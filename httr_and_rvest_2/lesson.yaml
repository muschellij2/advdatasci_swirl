- Class: meta
  Course: advdatasci_swirl
  Lesson: 03_02_parsing_html_tables
  Author: John Muschelli
  Type: Standard
  Organization: JHU Biostatistics and DSL
  Version: 2.4.2.3

- Class: text
  Output: Last time we talked about the httr and rvest packages and we 
    will be continuing on with some data collection and parsing.

- Class: cmd_question
  Output: Let's load in the httr package (it's already loaded) again.
  CorrectAnswer: library(httr)
  AnswerTests: omnitest(correctExpr='library(httr)')
  Hint: Type library(httr)

- Class: cmd_question
  Output: We'll use the GET command again to grab the html from the webpage 
    of the CDC "http://www.cdc.gov/vaccines/programs/vfc/awardees/vaccine-management/price-list/"
    and assign it to ret.
  CorrectAnswer: ret <- GET("http://www.cdc.gov/vaccines/programs/vfc/awardees/vaccine-management/price-list/")
  AnswerTests: omnitest(correctExpr='ret <- GET("http://www.cdc.gov/vaccines/programs/vfc/awardees/vaccine-management/price-list/")')
  Hint: ret <- GET("http://www.cdc.gov/vaccines/programs/vfc/awardees/vaccine-management/price-list/")

- Class: cmd_question
  Output: Again let's assign the content of ret to something, let's call it page.
  CorrectAnswer: page <- content(ret)
  AnswerTests: omnitest(correctExpr='page <- content(ret)')
  Hint: page <- content(ret)

- Class: cmd_question
  Output: Now, let's load in the rvest library again.
  CorrectAnswer: library(rvest)
  AnswerTests: omnitest(correctExpr='library(rvest)')
  Hint: library(rvest)

- Class: mult_question
  Output: We are going to grab the tables from the webpage 
    (http://www.cdc.gov/vaccines/programs/vfc/awardees/vaccine-management/price-list/), 
    how many should there be? (Look at the page)
  AnswerChoices: 1;2;3;4
  CorrectAnswer: 4
  AnswerTests: omnitest(correctVal='4')
  Hint: Look at the page

- Class: cmd_question
  Output: Now let's parse those tables with html_table from rvest, and assign that
    to the R object tabs.
  CorrectAnswer: tabs <- html_table(page)
  AnswerTests: omnitest(correctExpr='tabs <- html_table(page)')
  Hint: tabs <- html_table(page)

- Class: cmd_question
  Output: Print the length of tabs
  CorrectAnswer: length(tabs)
  AnswerTests: omnitest(correctExpr='length(tabs)')
  Hint: length(tabs)

- Class: cmd_question
  Output: Let's grab the 4th table (adult influenza vaccines) and assign it to flu.
  CorrectAnswer: flu <- tabs[[4]]
  AnswerTests: omnitest(correctExpr='flu <- tabs[[4]]')
  Hint: flu <- tabs[[4]]

- Class: cmd_question
  Output: Let's grab the column "Private Sector Cost/ Dose" of flu,
    using bracket ([, ]) notation, and put it into an 
    object called priv.
  CorrectAnswer: priv <- flu[, "Private Sector Cost/ Dose"]
  AnswerTests: omnitest(correctExpr='priv <- flu[, "Private Sector Cost/ Dose"]')
  Hint: priv <- flu[, "Private Sector Cost/ Dose"]

- Class: cmd_question
  Output: Let's look at the first few values of priv with head
  CorrectAnswer: head(priv)
  AnswerTests: omnitest(correctExpr='head(priv)')
  Hint: head(priv)
  
- Class: cmd_question
  Output: Let's try the mean of priv.
  CorrectAnswer: mean(priv)
  AnswerTests: omnitest(correctExpr='mean(priv)')
  Hint: mean(priv)

- Class: cmd_question
  Output: We see that it's NA, let's use the sub command to strip off the $ off of priv, 
    making sure to use fixed = TRUE, and re-assign it to priv.
  CorrectAnswer: priv <- sub("$", "", priv, fixed = TRUE)
  AnswerTests: omnitest(correctExpr='priv <- sub("$", "", priv, fixed = TRUE)')
  Hint: priv <- sub("$", "", priv, fixed = TRUE)

- Class: cmd_question
  Output: Let's look at another GET example.  Let's run the get command where
    the url is "https://twitter.com", path = "search", and query is a list with 
    a named element q which is equal to "monkey".  Assign this to the object monk
  CorrectAnswer: monk <- GET("https://twitter.com", path = "search", query = list(q = "monkey"))
  AnswerTests: omnitest(correctExpr='monk <- GET("https://twitter.com", path = "search", query = list(q = "monkey"))')
  Hint: monk <- GET("https://twitter.com", path = "search", query = list(q = "monkey"))

- Class: cmd_question
  Output: Let's look at the class of monk.
  CorrectAnswer: class(monk)
  AnswerTests: omnitest(correctExpr='class(monk)')
  Hint: class(monk)

- Class: cmd_question
  Output: Let's run the stop_for_status command on monk.  This will fail if the
    response was a failure or didn't execute correctly.
  CorrectAnswer: stop_for_status(monk)
  AnswerTests: omnitest(correctExpr='stop_for_status(monk)')
  Hint: stop_for_status(monk)
  
- Class: cmd_question
  Output: Let's get the content for monk and assign it to tweets.
  CorrectAnswer: tweets <- content(monk)
  AnswerTests: omnitest(correctExpr='tweets <- content(monk)')
  Hint: tweets <- content(monk)
  
- Class: cmd_question
  Output: Let's get all the html nodes with ".tweet-text" in the css and assign
    it to tweet_text.
  CorrectAnswer: tweet_text <- html_nodes(tweets, ".tweet-text")
  AnswerTests: omnitest(correctExpr='tweet_text <- html_nodes(tweets, ".tweet-text")')
  Hint: tweet_text <- html_nodes(tweets, ".tweet-text")
  
- Class: cmd_question
  Output: Let's get all the html nodes with ".tweet-text" in the css and assign
    it to tweet_text.
  CorrectAnswer: tweet_text <- html_nodes(tweets, ".tweet-text")
  AnswerTests: omnitest(correctExpr='tweet_text <- html_nodes(tweets, ".tweet-text")')
  Hint: tweet_text <- html_nodes(tweets, ".tweet-text")  

- Class: cmd_question
  Output: Let's get all the links to hashtags from tweet_text.  We will use 
    xml_find_all and some xpath "//a[ contains(@href, 'hashtag')]".  
    Let's assign this to hash.
  CorrectAnswer: hash <- xml_find_all(tweet_text, "//a[ contains(@href, 'hashtag')]")
  AnswerTests: expr_creates_var('hash')
  Hint: hash <- xml_find_all(tweet_text, "//a[ contains(@href, 'hashtag')]")
  
- Class: text
  Output: The xpath line "//a[ contains(@href, 'hashtag')]" means that you are 
    looking for the "a" tag anywhere (how you specify links in html) and an 
    attribute (href, which is referenced by an @ symbol contains a certain word,
    in this case hashtag.
    
- Class: cmd_question
  Output: Let's try to find the paragraphs that have the class "tweet-text.
    Let's again use xml_find_all and try the xpath "//p[@class = 'tweet-text']".  
  CorrectAnswer: xml_find_all(tweet_text, "//p[@class = 'tweet-text']")
  AnswerTests: calculates_same_value('structure(list(), class = "xml_nodeset")')
  Hint: xml_find_all(tweet_text, "//p[@class = 'tweet-text']")
  
- Class: text
  Output: Nothing came out.  That's because the paragraphs have MULTIPLE classes 
    and the syntax there tested that it exactly was that one class.

- Class: cmd_question
  Output: Let's try again, but using contains.  
    Let's again use xml_find_all and try the xpath "//p[@class = 'tweet-text') and @language = 'en']".  
  CorrectAnswer: xml_find_all(tweet_text, "//p[ contains(@class, 'tweet-text') and @language = 'en']")
  AnswerTests: omnitest(correctExpr='xml_find_all(tweet_text, "//p[ contains(@class, \'tweet-text\') and @language = \'en\']")')
  Hint: xml_find_all(tweet_text, "//p[ contains(@class, 'tweet-text') and @language = 'en']")   

- Class: text
  Output: Overall there are many more VERBs such as PUT/POST and such to upload 
    or post information to a website, and we touched on GET.  For web-scraping,
    you will be using GET a lot.  See the ?add_headers information to discuss
    headers you can add to a GET statement if you need to add headers to a call. 
    XPath is very powerful to extract information from the HTML output, but it is 
    pretty much a language onto iself.  Selector gadget helps, but you may need to
    do many repeated calls to html_nodes or xml_nodes to get the output you want.
    Happy scraping!

- Class: mult_question
  Output: Would you like to submit the log of this lesson to Google Forms so
    that your instructor may evaluate your progress?
  AnswerChoices: Yes;No
  CorrectAnswer: Yes
  AnswerTests: submit_log()
  Hint: ""
